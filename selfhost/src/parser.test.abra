// When executed directly, this will first perform the tokenization of the input file, and then will parse
// the file. It will then output the generated AST as JSON to stdout (code which results in a ParserError
// will result in the formatted error message being printed to stdout).
// This is split out into a separate runnable file so that the test-specific code is never compiled into
// the actual resulting binary; this results in a separate binary being compiled which is only used for
// testing.

import "fs" as fs
import Lexer from "./lexer"
import Parser from "./parser"
import printParsedModuleAsJson from "./test_utils"

if Process.args()[1] |fileName| {
  match fs.readFile(fileName) {
    Result.Ok(contents) => {
      match Lexer.tokenize(contents) {
        Result.Ok(tokens) => {
          match Parser.parse(tokens) {
            Result.Ok(parsedModule) => printParsedModuleAsJson(parsedModule)
            Result.Err(error) => print(error.getMessage(fileName, contents))
          }
        }
        Result.Err(error) => print(error.getMessage(fileName, contents))
      }
    }
    Result.Err(e) => {
      println("Could not read file:", e)
    }
  }
} else {
  println("Missing required argument <file-name>")
}

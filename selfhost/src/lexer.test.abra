// When executed directly, this will perform the tokenization and output the tokens as JSON to stdout.
// This is split out into a separate runnable file so that the test-specific code is never compiled into
// the actual resulting binary; this results in a separate binary being compiled which is only used for
// testing.

import "fs" as fs
import Token, TokenKind, Lexer from "./lexer"

func printTokensAsJson(tokens: Token[]) {
  println("[")

  val indent = "  "

  for token, idx in tokens {
    println("$indent{")
    println("$indent$indent\"position\": [${token.position.line}, ${token.position.col}],")
    print("$indent$indent\"kind\": ")
    printTokenKindAsJson(token.kind, "    ")
    val comma = if idx != tokens.length - 1 "," else ""
    println("${indent}}${comma}")
  }

  println("]")
}

func printTokenKindAsJson(kind: TokenKind, indent: String) {
  println("{")
  match kind {
    TokenKind.Int(value) => {
      println("$indent  \"name\": \"Int\",")
      println("$indent  \"value\": $value")
    }
    TokenKind.Ident(name) => {
      println("$indent  \"name\": \"Ident\",")
      println("$indent  \"value\": \"$name\"")
    }
    TokenKind.Dot => {
      println("$indent  \"name\": \"Dot\"")
    }
  }
  println("$indent}")
}

if Process.args()[1] |fileName| {
  match fs.readFile(fileName) {
    Result.Ok(contents) => {
      val tokens = Lexer.tokenize(contents)
      printTokensAsJson(tokens)
    }
    Result.Err(e) => {
      println("Could not read file:", e)
    }
  }
} else {
  println("Missing required argument <file-name>")
}

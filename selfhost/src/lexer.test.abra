// When executed directly, this will perform the tokenization and output the tokens as JSON to stdout.
// This is split out into a separate runnable file so that the test-specific code is never compiled into
// the actual resulting binary; this results in a separate binary being compiled which is only used for
// testing.

import "fs" as fs
import Token, TokenKind, Lexer from "./lexer"

export func printTokensAsJson(tokens: Token[]) {
  println("[")

  val indent = "  "

  for token, idx in tokens {
    println("$indent{")
    println("$indent$indent\"position\": [${token.position.line}, ${token.position.col}],")
    print("$indent$indent\"kind\": ")
    printTokenKindAsJson(token, "$indent$indent")
    val comma = if idx != tokens.length - 1 "," else ""
    println("${indent}}${comma}")
  }

  println("]")
}

func printTokenKindAsJson(token: Token, indent: String) {
  println("{")
  match token.kind {
    TokenKind.Int/*(value)*/ => {
      println("$indent  \"name\": \"Int\",")
      println("$indent  \"value\": ${token.intValue}")
    }
    TokenKind.Ident/*(name)*/ => {
      println("$indent  \"name\": \"Ident\",")
      println("$indent  \"value\": \"${token.strValue}\"")
    }
    TokenKind.Dot => {
      println("$indent  \"name\": \"Dot\"")
    }
  }
  println("$indent}")
}

if Process.args()[1] |fileName| {
  match fs.readFile(fileName) {
    Result.Ok(contents) => {
      val tokens = Lexer.tokenize(contents)
      printTokensAsJson(tokens)
    }
    Result.Err(e) => {
      println("Could not read file:", e)
    }
  }
} else {
  println("Missing required argument <file-name>")
}

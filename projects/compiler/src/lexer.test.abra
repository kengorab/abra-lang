// When executed directly, this will perform the tokenization and output the tokens as JSON to stdout (
// code which results in a LexerError will result in the formatted error message being printed to stdout).
// This is split out into a separate runnable file so that the test-specific code is never compiled into
// the actual resulting binary; this results in a separate binary being compiled which is only used for
// testing.

import "fs" as fs
import "process" as process
import Token, Lexer from "./lexer"
import printTokenAsJson from "./test_utils"

pub func printTokensAsJson(tokens: Token[]) {
  println("[")

  val indent = "  "

  for token, idx in tokens {
    printTokenAsJson(token, 1, 1)
    val comma = if idx != tokens.length - 1 "," else ""
    println(comma)
  }

  println("]")
}

if process.args()[1] |fileName| {
  match fs.readFile(fileName) {
    Ok(contents) => {
      match Lexer.tokenize(contents) {
        Ok(tokens) => printTokensAsJson(tokens)
        Err(error) => print(error.getMessage(fileName, contents))
      }
    }
    Err(e) => {
      println("Could not read file: $e")
    }
  }
} else {
  println("Missing required argument <file-name>")
}
